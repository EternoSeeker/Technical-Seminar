{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The research paper \"Parkinsonâ€™s Disease Detection Based on Spectrogram-Deep Convolutional Generative Adversarial Network Sample Augmentation\" utilized a Spectrogram Deep Convolutional Generative Adversarial Network (S-DCGAN) for sample augmentation to enhance voiceprint recognition accuracy. The training process involved using the Stochastic Gradient Descent algorithm (SGD) with a batch size of 16, Leaky ReLu slope of 0.2, and Adam optimizer with a learning rate of 0.0002. The S-DCGAN model was trained for 300 epochs, with the loss values converging to specific values for stability\n",
    "\n",
    "# S-DCGAN Model: Incorporates SN for all layers in the generator (G) and replaces Batch Norm with Spectral Norm in the discriminator (D).\n",
    "# ResNet50 Classifier: Utilized for voiceprint feature extraction and classification, enhancing recognition performance.\n",
    "# Optimizers and Hyperparameters:\n",
    "# Optimizer: Adam optimizer with a learning rate of 0.0002.\n",
    "# Batch Size: Set to 16 for training stability.\n",
    "# Leaky ReLu Slope: Maintained at 0.2 for activation functions.\n",
    "# Training Epochs: Trained for 300 epochs to achieve convergence.\n",
    "# Loss Functions:\n",
    "# Generator Loss: Minimizes error in generating fake samples and feature matching process.\n",
    "# Discriminator Loss: Maximizes network output in a preset manner.\n",
    "# write a code to train this model and evaluate it\n",
    "\n",
    "# assume the txt data file names as train_data and test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\SHRIHARI MAGAR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, Conv2D, Conv2DTranspose, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator network\n",
    "def build_generator(input_dim, alpha=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256 * 8 * 8, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Reshape((8, 8, 256)))\n",
    "    model.add(Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# Define the discriminator network\n",
    "def build_discriminator(alpha=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(64, 64, 1)))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Define the GAN model\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = generator.inputs\n",
    "    generated_image = generator.outputs\n",
    "    gan_output = discriminator(generated_image)\n",
    "    model = tf.keras.models.Model(gan_input, gan_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "# Normalize the data\n",
    "train_data = train_data / 255.0\n",
    "test_data = test_data / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1039, 29), (167, 28))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "alpha = 0.2\n",
    "batch_size = 16\n",
    "epochs = 300\n",
    "learning_rate = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile networks\n",
    "generator = build_generator(latent_dim, alpha)\n",
    "discriminator = build_discriminator(alpha)\n",
    "gan = build_gan(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(optimizer=Adam(learning_rate), loss='binary_crossentropy')\n",
    "gan.compile(optimizer=Adam(learning_rate), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, gan, train_data, test_data, latent_dim, batch_size, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Train discriminator\n",
    "        idx = np.random.randint(0, train_data.shape[0], batch_size)\n",
    "        X_train_real = tf.reshape(train_data[idx], (-1, 64, 64, 1))  # Reshape to match discriminator input shape\n",
    "        X_train_fake = generator.predict(np.random.normal(0, 1, (batch_size, latent_dim)))\n",
    "        X_train = tf.concat([X_train_real, X_train_fake], axis=0)\n",
    "        y_train = np.concatenate((np.ones((batch_size, 1)), np.zeros((batch_size, 1))))\n",
    "        discriminator.trainable = True\n",
    "        d_loss = discriminator.train_on_batch(X_train, y_train)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        y_gan = np.ones((batch_size, 1))\n",
    "        discriminator.trainable = False\n",
    "        g_loss = gan.train_on_batch(noise, y_gan)\n",
    "\n",
    "        # Print losses\n",
    "        print(f\"Epoch: {epoch+1}/{epochs}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n",
    "\n",
    "    train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100)\n",
      "(None, 64, 64, 1)\n",
      "(None, 100)\n"
     ]
    }
   ],
   "source": [
    "print(generator.input_shape)\n",
    "print(discriminator.input_shape)\n",
    "print(gan.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([1013,   88,   44,  688,  361, 1008,  962,  536,  263,  472,  736,\n         59,  604,  364,   51,  253])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(generator, discriminator, gan, train_data, test_data, latent_dim, batch_size, epochs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Train discriminator\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, train_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], batch_size)\n\u001b[1;32m----> 5\u001b[0m     X_train_real \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Reshape to match discriminator input shape\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     X_train_fake \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (batch_size, latent_dim)))\n\u001b[0;32m      7\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([X_train_real, X_train_fake], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SHRIHARI MAGAR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\SHRIHARI MAGAR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:961\u001b[0m, in \u001b[0;36m_check_index\u001b[1;34m(idx)\u001b[0m\n\u001b[0;32m    956\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _SUPPORTED_SLICE_DTYPES \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    958\u001b[0m     idx\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(idx\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    959\u001b[0m   \u001b[38;5;66;03m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[39;00m\n\u001b[0;32m    960\u001b[0m   \u001b[38;5;66;03m# will break `_slice_helper` contract.\u001b[39;00m\n\u001b[1;32m--> 961\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(_SLICE_TYPE_ERROR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx))\n",
      "\u001b[1;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([1013,   88,   44,  688,  361, 1008,  962,  536,  263,  472,  736,\n         59,  604,  364,   51,  253])"
     ]
    }
   ],
   "source": [
    "train_gan(generator, discriminator, gan, train_data, test_data, latent_dim, batch_size, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
